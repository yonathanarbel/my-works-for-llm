# my-works-for-llm

Machine-readable corpus of Professor Yonathan Arbel's scholarship for LLM research. The
repository packages lightly processed versions of each paper alongside synthetic article
scripts so the works can be explored programmatically or ingested into downstream large
language model (LLM) pipelines.

## Why this corpus matters

- **Unified access to scholarship.** All of Professor Arbel's public writing in one place,
  normalized to UTF-8 text to simplify ingestion.
- **Training-ready assets.** The generated article scripts print rich-formatted Markdown
  that can be streamed directly into tokenizers during dataset preparation.
- **Machine-readable metadata.** A Schema.org `Dataset` description and sitemap make it
  easier to reference or publish the collection.

## Repository layout

```
.
├── article_scripts/      # Autogenerated rich-print scripts wrapping each paper summary
├── papers/               # Canonical paper folders with `summary.md` and/or `paper.txt`
├── dataset.jsonld        # Schema.org metadata for discoverability
├── generate_article_scripts.py
├── llms.txt              # Quick links to hosted models featuring the scholarship
└── sitemap.xml           # Machine-readable index of the collection
```

## Getting started

1. **Install dependencies.** The generator relies on `rich` for colorful debug output:

   ```bash
   pip install rich
   ```

2. **Refresh the article scripts.**

   ```bash
   python generate_article_scripts.py
   ```

   The script scans `papers/`, prefers `summary.md` when present, and otherwise falls back
   to `paper.txt`. Each generated module lands in `article_scripts/` and prints the
   Markdown payload with `rich`.

3. **Stream content into an LLM pipeline.** Example snippet:

   ```python
   from importlib.machinery import SourceFileLoader

   loader = SourceFileLoader("ssrn_3519630", "article_scripts/ssrn-3519630.py")
   module = loader.load_module()
   text = module.article_text
   tokenizer.encode(text)
   ```

   The scripts also emit the text when executed directly, which can be helpful for
   spot-checking formatting or piping the output into preprocessing utilities.

## Preparing data for training

- **Chunking.** Each Markdown document contains logical sections (e.g., `## TL;DR`,
  `## Section Summaries`). Use these headings to split the content into semantically rich
  passages.
- **Metadata tagging.** Combine filenames, SSRN identifiers, and the metadata in
  `dataset.jsonld` to construct provenance-aware training examples.
- **Augmentation.** Consider mixing the human-authored summaries with the full paper text
  located in `papers/*/paper.txt` for multi-granularity corpora.
- **Evaluation sets.** Reserve a subset of summaries (e.g., specific SSRN IDs) for
  evaluation when benchmarking retrieval or question-answering tasks.

## Contributing improvements

1. Add or update a paper inside `papers/<ssrn-id>/`.
2. Run `python generate_article_scripts.py` to regenerate the paired script.
3. Update `dataset.jsonld` if new topical areas or licensing terms are introduced.
4. Submit a pull request detailing the additions and any preprocessing considerations.

## Licensing

All textual materials are distributed under the Creative Commons Attribution-NonCommercial
4.0 International License. See `LICENSE` for details and ensure downstream datasets are
tagged with equivalent restrictions.
