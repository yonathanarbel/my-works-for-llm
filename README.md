# my-works-for-llm

Machine-readable corpus of Professor Yonathan Arbel's scholarship for LLM research. The
repository packages lightly processed versions of each paper alongside synthetic article
scripts so the works can be explored programmatically or ingested into downstream large
language model (LLM) pipelines.

**ðŸ“š New to this repository?** Start with the [Quick Start Guide](QUICK_START.md) | [Usage Examples](USAGE_EXAMPLES.md) | [Jupyter Notebook](examples/explore_corpus.ipynb)

## Citation & discovery

- **Citation:** `CITATION.cff` (GitHub â€œCite this repositoryâ€ support).
- **Zenodo DOI:** see `ZENODO.md` (uses `.zenodo.json` for release metadata).
- **Sitemap:** `sitemap.xml` is generated by `generate_sitemap.py` and includes summaries, full text, and per-paper Schema.org JSON-LD.
- **GitHub Pages:** `docs/` is generated by `generate_docs_site.py` (per-paper pages with OG tags + embedded JSON-LD, plus Atom feed).
- **Study assets:** `generate_paper_assets.py` creates per-paper one-pagers, study packs, and code wrappers for code-centric ingestion.
- **Exports:** `export_llm_dataset.py` (JSONL/Markdown) and `build_rag_chunks.py` (RAG chunk JSONL).
- **Hugging Face:** `publish_hf_dataset.py` uploads a dataset snapshot (requires `huggingface_hub` + token).

## Why this corpus matters

- **Unified access to scholarship.** All of Professor Arbel's public writing in one place,
  normalized to UTF-8 text to simplify ingestion.
- **Training-ready assets.** The generated article scripts print rich-formatted Markdown
  that can be streamed directly into tokenizers during dataset preparation.
- **Machine-readable metadata.** A Schema.org `Dataset` description and sitemap make it
  easier to reference or publish the collection.
- **Powerful tooling.** Command-line interface and Python API for easy exploration,
  searching, and programmatic access to papers.
- **Quality assurance.** Automated validation ensures corpus integrity and consistency.

## Repository layout

```
.
â”œâ”€â”€ article_scripts/      # Autogenerated rich-print scripts wrapping each paper summary
â”œâ”€â”€ papers/               # Canonical paper folders with `summary.md` and/or `paper.txt`
â”œâ”€â”€ examples/             # Jupyter notebooks and example scripts
â”œâ”€â”€ corpus_api.py         # Python API for programmatic corpus access
â”œâ”€â”€ corpus_cli.py         # Command-line tool for exploring the corpus
â”œâ”€â”€ dataset.jsonld        # Schema.org metadata for discoverability
â”œâ”€â”€ generate_article_scripts.py
â”œâ”€â”€ generate_paper_assets.py
â”œâ”€â”€ generate_paper_jsonld.py
â”œâ”€â”€ generate_sitemap.py
â”œâ”€â”€ generate_docs_site.py
â”œâ”€â”€ export_llm_dataset.py
â”œâ”€â”€ build_rag_chunks.py
â”œâ”€â”€ publish_hf_dataset.py
â”œâ”€â”€ CITATION.cff
â”œâ”€â”€ .zenodo.json
â”œâ”€â”€ ZENODO.md
â”œâ”€â”€ docs/                # GitHub Pages site (generated)
â”œâ”€â”€ huggingface/         # Hugging Face dataset card template
â”œâ”€â”€ llms.txt              # Quick links to hosted models featuring the scholarship
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ setup.py              # Package installation configuration
â”œâ”€â”€ QUICK_START.md        # Quick start guide for new users
â”œâ”€â”€ USAGE_EXAMPLES.md     # Detailed usage examples
â””â”€â”€ sitemap.xml           # Machine-readable index of the collection
```

## Getting started

### Quick Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or install as a package with CLI tools
pip install -e .
```

### Using the Command-Line Interface

The corpus includes a powerful CLI tool for exploring papers:

```bash
# List all papers in the corpus
python corpus_cli.py list

# Search for papers about specific topics
python corpus_cli.py search "contract"

# Show detailed information about a paper
python corpus_cli.py show ssrn-3519630

# Display corpus statistics
python corpus_cli.py stats

# Validate corpus integrity
python corpus_cli.py validate
```

If installed via `pip install -e .`, you can use the shorter command:

```bash
arbel-corpus list
arbel-corpus search "artificial intelligence"
arbel-corpus show ssrn-3519630 --content
```

### Using the Python API

For programmatic access to the corpus:

```python
from corpus_api import ArbelCorpus

# Load the corpus
corpus = ArbelCorpus()

# Get all papers
papers = corpus.list_papers()
print(f"Found {len(papers)} papers")

# Get a specific paper
paper = corpus.get_paper('ssrn-3519630')
print(paper.get_title())

# Read paper content
summary = paper.get_summary()  # English summary
summary_zh = paper.get_summary('zh')  # Chinese summary
full_text = paper.get_full_text()  # Full paper text

# Search papers
results = corpus.search_papers("artificial intelligence")
for paper in results:
    print(f"{paper.paper_id}: {paper.get_title()}")

# Iterate through all papers
for paper in corpus.iterate_papers():
    if paper.has_summary():
        print(paper.get_title())
```

### Generating Article Scripts

Refresh the article scripts after adding new papers:

```bash
python generate_article_scripts.py
```

The script scans `papers/`, prefers `summary.md` when present, and otherwise falls back
to `paper.txt`. Each generated module lands in `article_scripts/` and prints the
Markdown payload with `rich`.

### Streaming into LLM Pipelines

**Method 1: Using the Python API (Recommended)**

```python
from corpus_api import ArbelCorpus

corpus = ArbelCorpus()
for paper in corpus.iterate_papers():
    text = paper.get_summary() or paper.get_full_text()
    if text:
        tokens = tokenizer.encode(text)
        # Process tokens...
```

**Method 2: Using article scripts**

```python
from importlib.machinery import SourceFileLoader

loader = SourceFileLoader("ssrn_3519630", "article_scripts/ssrn-3519630.py")
module = loader.load_module()
text = module.article_text
tokenizer.encode(text)
```

The scripts also emit the text when executed directly, which can be helpful for
spot-checking formatting or piping the output into preprocessing utilities.

## Advanced Features

### Corpus Statistics

Get detailed statistics about the corpus:

```bash
python corpus_cli.py stats
```

This displays:
- Total number of papers
- Papers with English/Chinese summaries
- Papers with metadata
- File type distribution
- Total corpus size

### Validation

Ensure corpus integrity:

```bash
python corpus_cli.py validate
```

This checks for:
- Missing required files (summary.md or paper.txt)
- Orphaned or missing article scripts
- Invalid JSON in metadata files

### Search and Filter

Search across papers:

```bash
# Search all fields
python corpus_cli.py search "contract law"

# Search specific fields
python corpus_cli.py search "disclosure" --field abstract
```

Or use the Python API:

```python
corpus = ArbelCorpus()
results = corpus.search_papers("artificial intelligence", field="title")
```

### Output Formats

The CLI supports multiple output formats:

```bash
# Table format (default)
python corpus_cli.py list

# Simple list
python corpus_cli.py list --format simple

# JSON output for automation
python corpus_cli.py list --format json > papers.json
```

## Preparing data for training

- **Chunking.** Each Markdown document contains logical sections (e.g., `## TL;DR`,
  `## Section Summaries`). Use these headings to split the content into semantically rich
  passages.
- **Metadata tagging.** Combine filenames, SSRN identifiers, and the metadata in
  `dataset.jsonld` to construct provenance-aware training examples.
- **Augmentation.** Consider mixing the human-authored summaries with the full paper text
  located in `papers/*/paper.txt` for multi-granularity corpora.
- **Evaluation sets.** Reserve a subset of summaries (e.g., specific SSRN IDs) for
  evaluation when benchmarking retrieval or question-answering tasks.

## Contributing improvements

1. Add or update a paper inside `papers/<ssrn-id>/`.
2. Run `python generate_article_scripts.py` to regenerate the paired script.
3. Update `dataset.jsonld` if new topical areas or licensing terms are introduced.
4. Submit a pull request detailing the additions and any preprocessing considerations.

## Licensing

All textual materials are distributed under the Creative Commons Attribution-NonCommercial
4.0 International License. See `LICENSE` for details and ensure downstream datasets are
tagged with equivalent restrictions.
