{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Professor Arbel's Scholarship Corpus\n",
    "\n",
    "This notebook demonstrates how to explore and work with the corpus using the Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory to path\n",
    "\n",
    "from corpus_api import ArbelCorpus\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus from parent directory\n",
    "corpus = ArbelCorpus(base_dir='..')\n",
    "print(f\"Loaded corpus with {len(corpus)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = corpus.get_stats()\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = corpus.list_papers()\n",
    "\n",
    "print(f\"Found {len(papers)} papers:\\n\")\n",
    "for paper in papers[:5]:  # Show first 5\n",
    "    print(f\"- {paper.paper_id}: {paper.get_title()}\")\n",
    "    print(f\"  Authors: {', '.join(paper.get_authors())}\")\n",
    "    print(f\"  Year: {paper.get_year()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for papers about contracts\n",
    "results = corpus.search_papers(\"contract\")\n",
    "\n",
    "print(f\"Found {len(results)} papers about contracts:\\n\")\n",
    "for paper in results[:3]:  # Show first 3\n",
    "    print(f\"- {paper.paper_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore a Specific Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific paper\n",
    "paper = corpus.get_paper('ssrn-3519630')\n",
    "\n",
    "print(f\"Paper ID: {paper.paper_id}\")\n",
    "print(f\"Title: {paper.get_title()}\")\n",
    "print(f\"Authors: {', '.join(paper.get_authors())}\")\n",
    "print(f\"Year: {paper.get_year()}\")\n",
    "print(f\"\\nAvailable files: {', '.join(paper.get_available_files())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Paper Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English summary\n",
    "summary = paper.get_summary()\n",
    "\n",
    "if summary:\n",
    "    print(\"Summary (first 500 characters):\\n\")\n",
    "    print(summary[:500] + \"...\\n\")\n",
    "    print(f\"Full summary length: {len(summary)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Paper Distribution by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "years = [p.get_year() for p in corpus.list_papers() if p.get_year()]\n",
    "year_counts = Counter(years)\n",
    "\n",
    "print(\"Papers by year:\")\n",
    "for year, count in sorted(year_counts.items()):\n",
    "    print(f\"{year}: {'â–ˆ' * count} ({count} papers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Bilingual Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_papers = [\n",
    "    p for p in corpus.list_papers() \n",
    "    if p.has_summary('en') and p.has_summary('zh')\n",
    "]\n",
    "\n",
    "print(f\"Found {len(bilingual_papers)} papers with both English and Chinese summaries\")\n",
    "print(f\"\\nPercentage: {len(bilingual_papers) / len(corpus) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple training dataset\n",
    "training_data = []\n",
    "\n",
    "for paper in corpus.iterate_papers():\n",
    "    summary = paper.get_summary()\n",
    "    if summary:\n",
    "        training_data.append({\n",
    "            'id': paper.paper_id,\n",
    "            'title': paper.get_title(),\n",
    "            'text': summary,\n",
    "            'year': paper.get_year()\n",
    "        })\n",
    "\n",
    "print(f\"Created training dataset with {len(training_data)} examples\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(json.dumps(training_data[0], indent=2)[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_lengths = []\n",
    "\n",
    "for paper in corpus.iterate_papers():\n",
    "    summary = paper.get_summary()\n",
    "    if summary:\n",
    "        word_count = len(summary.split())\n",
    "        summary_lengths.append((paper.paper_id, word_count))\n",
    "\n",
    "# Sort by length\n",
    "summary_lengths.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 5 longest summaries:\\n\")\n",
    "for paper_id, word_count in summary_lengths[:5]:\n",
    "    print(f\"{paper_id}: {word_count:,} words\")\n",
    "\n",
    "print(f\"\\nAverage summary length: {sum(w for _, w in summary_lengths) / len(summary_lengths):,.0f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all summaries to a JSON file\n",
    "export_data = []\n",
    "\n",
    "for paper in corpus.iterate_papers():\n",
    "    summary = paper.get_summary()\n",
    "    if summary:\n",
    "        export_data.append({\n",
    "            'paper_id': paper.paper_id,\n",
    "            'title': paper.get_title(),\n",
    "            'authors': paper.get_authors(),\n",
    "            'year': paper.get_year(),\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "# Save to file\n",
    "with open('corpus_export.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Exported {len(export_data)} papers to corpus_export.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore individual papers in detail\n",
    "- Build custom datasets for your use case\n",
    "- Integrate with NLP pipelines\n",
    "- Create visualizations of the corpus\n",
    "\n",
    "See `USAGE_EXAMPLES.md` for more examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
